# Wikipedia Talk Page Policy Analyzer

A web application that analyzes Wikipedia talk page discussions and identifies policy, guideline, and essay mentions by directly extracting Wikipedia links from the discussion content.

## ğŸŒŸ Features

* **ğŸ“„ Two-Column Layout**: Discussion content on the left, policy mentions on the right
* **ğŸ” Direct Link Extraction**: Scans discussions for Wikipedia policy shortcuts (WP:NPOV, WP:RS, etc.)
* **ğŸ“ Section-Specific Analysis**: Analyzes specific discussion sections using URL anchors
* **ğŸ¨ Modern UI**: Beautiful, responsive design with smooth scrolling
* **âš¡ Fast & Accurate**: No AI guessing - directly extracts actual policy links

## ğŸ—ï¸ Architecture

The project is organized into clean, modular components:

```
Wikipedia/
â”œâ”€â”€ main.py                  # Flask application & routes
â”œâ”€â”€ scraper.py               # Wikipedia page scraping & section extraction
â”œâ”€â”€ policy_extractor.py      # Direct policy/guideline/essay link extraction
â”œâ”€â”€ analyzer.py              # OpenAI integration (optional)
â”œâ”€â”€ prompts.py               # AI prompt templates (optional)
â”œâ”€â”€ static/
â”‚   â””â”€â”€ style.css           # Frontend styling
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html          # Frontend HTML
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ setup.sh                # Setup script
```

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/YEETlord247/WIkipedia-Policy-Scraping.git
cd WIkipedia-Policy-Scraping
```

### 2. Run Setup Script

```bash
chmod +x setup.sh
./setup.sh
```

Or manually:

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create .env file (optional, for OpenAI features)
echo "OPENAI_API_KEY=your-api-key-here" > .env
```

### 3. Run the Application

```bash
python main.py
```

The application will start on **http://127.0.0.1:5001**

## ğŸ’¡ Usage

1. Open your web browser and navigate to `http://127.0.0.1:5001`
2. Paste a Wikipedia talk page URL with a section anchor:
   ```
   https://en.wikipedia.org/wiki/Talk:Article#Section_Name
   ```
3. Click "Analyze Discussion"
4. View the discussion on the left and identified policies on the right

### Example URLs

* `https://en.wikipedia.org/wiki/Talk:List_of_2022_FIFA_World_Cup_controversies#RfC_self-published_cartoon`
* `https://en.wikipedia.org/wiki/Talk:Dinosaur#Taxonomy`
* `https://en.wikipedia.org/wiki/Talk:Climate_change#Recent_edits`

## ğŸ”§ How It Works

1. **URL Parsing**: Extracts the section anchor from the provided URL
2. **Web Scraping**: Fetches the Wikipedia talk page using BeautifulSoup
3. **Section Extraction**: Isolates the specific discussion section
4. **Policy Detection**: Scans for Wikipedia policy/guideline links:
   - HTML links to `wikipedia.org/wiki/Wikipedia:*`
   - Text shortcuts like `WP:NPOV`, `WP:RS`, `WP:UNDUE`
5. **Categorization**: Classifies each into:
   - **ğŸ”´ Policies**: Mandatory rules (NPOV, BLP, NOTCENSORED)
   - **ğŸŸ¡ Guidelines**: Best practices (RS, UNDUE, MOS)
   - **ğŸ”µ Essays**: Opinion pieces (IAR, COMMON, DEADLINE)

## ğŸ“‹ Requirements

* Python 3.7+
* Internet connection

## ğŸ“¦ Dependencies

* **Flask**: Web framework
* **BeautifulSoup4**: Web scraping
* **Requests**: HTTP library
* **lxml**: HTML/XML parser
* **python-dotenv**: Environment variable management
* **openai** (optional): For AI-powered analysis features

## ğŸ¯ Project Goals

This tool was created to help Wikipedia editors:
- Quickly identify which policies are being discussed
- Understand the context of policy references
- Facilitate policy-focused discussions
- Research patterns in Wikipedia governance

## ğŸ¤ Contributing

Contributions are welcome! Feel free to:
- Report bugs
- Suggest new features
- Submit pull requests

## ğŸ“ License

This project is open source and available for educational and research purposes.

## ğŸ™ Acknowledgments

Built with â¤ï¸ for the Wikipedia community.

---

**Note**: This tool is for educational purposes and is not affiliated with the Wikimedia Foundation.
