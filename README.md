# Wikipedia Talk Page Policy Analyzer

A professional Flask web application that analyzes Wikipedia talk page discussions and identifies mentions of Wikipedia policies, guidelines, and essays with contextual information.

## ğŸ¯ Features

- **Precise Section Extraction**: Scrapes specific discussion sections from Wikipedia talk pages using the Wikipedia API
- **Comprehensive Policy Detection**: Identifies Wikipedia policies, guidelines, and essays through:
  - Direct Wikipedia links (e.g., `[[WP:NPOV]]`)
  - Shortcut codes (e.g., `WP:NOTCENSORED`, `WP:3RR`)
  - Full name matching (e.g., "Neutral Point of View")
- **Contextual Display**: Shows sentence-level context for each policy mention
- **Interactive Navigation**: Click any policy mention to scroll to and highlight it in the discussion
- **Clean UI**: Modern, responsive interface with organized panels

## ğŸ—ï¸ Project Structure

```
Wikipedia/
â”œâ”€â”€ app/                        # Flask application package
â”‚   â”œâ”€â”€ __init__.py            # App factory
â”‚   â”œâ”€â”€ routes.py              # HTTP endpoints
â”‚   â””â”€â”€ utils.py               # Helper functions
â”œâ”€â”€ scrapers/                   # Web scraping modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ html_scraper.py        # HTML-based scraper (legacy)
â”‚   â””â”€â”€ wikitext_scraper.py    # Wikipedia API scraper (active)
â”œâ”€â”€ analyzers/                  # Analysis modules
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ policy_extractor.py    # Policy detection & categorization
â”‚   â”œâ”€â”€ context_extractor.py   # Context sentence extraction
â”‚   â””â”€â”€ openai_analyzer.py     # OpenAI integration (optional)
â”œâ”€â”€ config/                     # Configuration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ prompts.py             # AI prompt templates
â”œâ”€â”€ static/                     # Frontend assets
â”‚   â””â”€â”€ style.css              # Application styles
â”œâ”€â”€ templates/                  # HTML templates
â”‚   â””â”€â”€ index.html             # Main interface
â”œâ”€â”€ main.py                     # Application entry point
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ render.yaml                # Render deployment config
â””â”€â”€ .env                       # Environment variables (not in repo)
```

## ğŸš€ Quick Start

### Local Development

1. **Clone the repository**
   ```bash
   git clone https://github.com/YEETlord247/WIkipedia-Policy-Scraping.git
   cd WIkipedia-Policy-Scraping
   ```

2. **Set up virtual environment**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Create `.env` file** (optional, for OpenAI features)
   ```bash
   echo "OPENAI_API_KEY=your_api_key_here" > .env
   ```

5. **Run the application**
   ```bash
   python main.py
   ```

6. **Open in browser**
   ```
   http://localhost:5001
   ```

### Production Deployment (Render)

1. **Push to GitHub**
   ```bash
   git add .
   git commit -m "Professional refactor"
   git push origin main
   ```

2. **Connect to Render**
   - Create a new Web Service on [Render](https://render.com)
   - Connect your GitHub repository
   - Render will auto-detect `render.yaml` configuration

3. **Set environment variables** (in Render dashboard)
   - `OPENAI_API_KEY`: Your OpenAI API key (if using AI features)

4. **Deploy**
   - Render will automatically build and deploy your application

## ğŸ“– Usage

1. **Navigate to the application** in your web browser

2. **Enter a Wikipedia talk page URL** with a section anchor:
   ```
   https://en.wikipedia.org/wiki/Talk:Article_Name#Section_Name
   ```

3. **Click "Analyze Discussion"**

4. **View results** in three organized panels:
   - **Left**: Full discussion content
   - **Right Top**: Detected policies with context
   - **Right Middle**: Detected guidelines with context
   - **Right Bottom**: Detected essays with context

5. **Click any policy mention** in the right panel to:
   - Auto-scroll to its location in the discussion
   - Highlight the mention for 3 seconds

## ğŸ”§ Configuration

### Scraper Mode

In `app/routes.py`, you can toggle between scraping modes:

```python
# Use 'wikitext' for Wikipedia API (recommended)
# Use 'html' for direct HTML scraping (legacy)
SCRAPER_MODE = 'wikitext'
```

### Policy Dictionary

The comprehensive policy/guideline/essay dictionary is maintained in `analyzers/policy_extractor.py` under the `WIKIPEDIA_ITEMS` constant.

## ğŸ› ï¸ Tech Stack

- **Backend**: Flask 3.0.0
- **Web Scraping**: BeautifulSoup4, Requests, Wikipedia API
- **AI Integration**: OpenAI API (optional)
- **Frontend**: HTML5, CSS3, Vanilla JavaScript
- **Deployment**: Gunicorn, Render
- **Development**: Python 3.9+

## ğŸ“¦ Dependencies

See `requirements.txt` for full list. Key dependencies:

- `flask==3.0.0` - Web framework
- `beautifulsoup4==4.12.2` - HTML parsing
- `requests==2.31.0` - HTTP client
- `lxml==5.1.0` - XML/HTML processing
- `python-dotenv==1.0.0` - Environment management
- `gunicorn==21.2.0` - Production WSGI server
- `openai>=1.50.0` - AI integration (optional)

## ğŸ¤ Contributing

This is a research prototype. Contributions, issues, and feature requests are welcome!

## ğŸ“ License

MIT License - feel free to use this project for your own purposes.

## ğŸ‘¨â€ğŸ’» Author

**Utkarsh Rai**

- GitHub: [@YEETlord247](https://github.com/YEETlord247)
- Repository: [Wikipedia-Policy-Scraping](https://github.com/YEETlord247/WIkipedia-Policy-Scraping)

## ğŸ™ Acknowledgments

- Wikipedia for their comprehensive API
- The Flask and Python communities
- BeautifulSoup4 for excellent HTML parsing

---

Built with â¤ï¸ for Wikipedia research and analysis
